{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#overview-and-introduction","title":"Overview and Introduction","text":"<p>Welcome to the CPyET Package Documentation! This document provides an overview of CPyET,  a powerful framework for working with cardiopulmonary exercise testing (CPET) signals via Python.  CPET is a widely used method for assessing cardiovascular and pulmonary function during exercise.  It involves monitoring various physiological signals, such as heart rate, oxygen consumption,  and breathing patterns, to evaluate an individual's fitness level and diagnose  certain cardiovascular and respiratory conditions.</p> <p>Analyzing CPET signals can provide valuable insights into the dynamic changes  that occur within the cardiopulmonary system during exercise.  It helps researchers, clinicians, and fitness professionals understand the  efficiency, limitations, and adaptive responses of the cardiovascular and respiratory systems.  By examining the patterns and variability of these signals, researchers can gain a deeper understanding of physiological mechanisms, identify abnormalities or impairments,  and monitor the effectiveness of interventions or training programs.</p> <p>The CPyET package is designed to streamline the variability analysis of CPET signals in Python. It provides end-to-end functionality, starting from constructing stationary signals  to determining appropriate metric parameters and efficiently computing entropy  and variability measures. By leveraging CPyET, researchers and practitioners can  focus on the analysis and interpretation of CPET data, rather than spending time  and effort on the intricate details of signal processing and analysis.</p> <p>To the best of our knowledge, CPyET is the only existing solution in Python that  offers all the necessary functionality for valid and reproducible CPET analysis  using novel and scalable heuristics. Its features and benefits enable researchers  to perform comprehensive variability analysis, gain valuable insights, and  contribute to advancements in the field of cardiopulmonary exercise testing.</p>"},{"location":"#features-benefits","title":"Features &amp; Benefits","text":"<p>CPyET offers a range of features and benefits that facilitate the analysis of CPET signals:</p> <ul> <li>Automatic Signal Stationarity: CPyET enables seamless construction of stationary signals,  a necessary condition for valid entropy and variability analysis.  It incorporates two common techniques, differencing and de-trending,  and performs statistical stationarity checks to ensure that the dataset contains valid signals.</li> <li>Scalable Entropy Calculations: CPyET provides efficient implementations of  sample and permutation entropy. Leveraging Numba's just-in-time compilation scheme,  CPyET ensures fast and scalable computations, allowing researchers to focus on  the analysis rather than the intricacies of the calculations.</li> <li>Optimal Parameter Selction: Determining appropriate parameter settings for  entropy measures can be challenging. CPyET takes the guesswork out by providing  reasonable recommendations based on rigorous, nonparametric statistical approaches.  These recommendations empower researchers to confidently choose suitable parameters for their analysis.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>CPyET is not yet available on PyPI. However, once it is published, you will be able to install it using pip:</p> <pre><code>pip install cpyet\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>To start using CPyET in your Python project, import it as follows:</p> <pre><code>import cpyet\n</code></pre>"},{"location":"#license","title":"License","text":"<p>CPyET is released under the MIT License.</p> <p>The MIT License (MIT)</p> <p>Copyright (c) 2023 Zachary Blanks</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/stationarity/","title":"Stationarity","text":"<p>Welcome to the documentation for the Stationarity module within the CPyET package.  This module provides functions for constructing stationary CPET signals,  and determining the proportion of stationary signals in a given dataset.</p>"},{"location":"api/stationarity/#functions","title":"Functions","text":""},{"location":"api/stationarity/#make_stationary_signals","title":"<code>make_stationary_signals</code>","text":"<p>Create stationary signals using the specified method.</p>"},{"location":"api/stationarity/#description","title":"Description","text":"<p>This function takes an input DataFrame containing signals and applies a specified method  to make the signals stationary. Stationarity is a necessary condition for valid entropy and variability analysis. The function either de-trends or differences a given signal to, ideally, yield statistically stationary signals.</p>"},{"location":"api/stationarity/#syntax","title":"Syntax","text":"<pre><code>make_stationary_signals(\ndf: pd.DataFrame,\nmethod: str,\ndetrend_type: str = \"gp\",\nsignal_id: str = \"signal_id\",\ntimestamp: str = \"timestamp\",\nvalue_col: str = \"value\",\nalpha: float = 0.05,\nrandom_seed: int = None,\nls_range: tuple[float, float] = (1.0, 100.0),\nn_searches: int = 10,\nn_splits: int = 5,\neps: float = 1e-6,\ngp_implementation: str = \"sklearn\",\n) -&gt; pd.DataFrame:\n</code></pre>"},{"location":"api/stationarity/#args","title":"Args","text":"<ul> <li><code>df</code> (pd.DataFrame): The input DataFrame containing the signals.</li> <li><code>method</code> (str): The method to make the signals stationary. Valid options are 'difference' and 'detrend'.</li> <li><code>detrend_type</code> (str, optional): The type of detrending. Required when the method is 'detrend'. Valid options are 'lr' and 'gp'. Default is 'gp'.</li> <li><code>signal_id</code> (str, optional): The column name for the signal ID. Default is 'signal_id'.</li> <li><code>timestamp</code> (str, optional): The column name for the timestamp. Default is 'timestamp'.</li> <li><code>value_col</code> (str, optional): The column name for the signal values. Default is 'value'.</li> <li><code>alpha</code> (float, optional): The significance level for the stationarity test. Must be in the range (0, 1). Default is 0.05.</li> <li><code>random_seed</code> (int, optional): The random seed for generating ls_vals. Must be an integer. Default is None.</li> <li><code>ls_range</code> (tuple[float, float], optional): The range of ls values for GP detrending. The lower bound must be greater than 0. Default is (1.0, 100.0).</li> <li><code>n_searches</code> (int, optional): The number of ls values to consider for GP detrending. Must be a positive integer. Default is 10.</li> <li><code>n_splits</code> (int, optional): The number of cross-validation splits. Must be a positive integer. Default is 5.</li> <li><code>eps</code> (float, optional): Small value added to the kernel matrix for numerical stability. Default is 1e-6.</li> <li><code>gp_implementation</code> (str, optional): Which GP detrending to use when the method is 'detrend' and detrend_type is 'gp'. Valid options are 'sklearn' and 'numba'. Default is 'sklearn'.</li> </ul>"},{"location":"api/stationarity/#returns","title":"Returns","text":"<ul> <li><code>pd.DataFrame</code>: The DataFrame with stationary signals.</li> </ul>"},{"location":"api/stationarity/#notes","title":"Notes","text":"<p>We have implemented an RBF-based GP detrending in Numba using a standard Cholesky factorization solution. This method, on average, is faster than the Scikit-Learn implementation. The benefit will become more noticeable as the number of unique signals, \\(N\\), increases. However, this implementation is less numerically stable and less well-tested than the Scikit-Learn version. You can control which version you use via the argument <code>gp_implementation</code>.</p>"},{"location":"api/stationarity/#example","title":"Example","text":"<pre><code>&gt;&gt;&gt; signal_ids = np.repeat([\"abc\", \"def\"], 100)\n&gt;&gt;&gt; timestamps = np.tile(np.arange(100), 2)\n&gt;&gt;&gt; rng = np.random.default_rng(17)\n&gt;&gt;&gt; abc_values = rng.uniform(-5, 5, size=(100,))\n&gt;&gt;&gt; def_values = rng.uniform(-5, 5, size=(100,))\n&gt;&gt;&gt; values = np.concatenate((abc_values, def_values))\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"signal_id\": signal_ids,\n...     \"timestamp\": timestamps,\n...     \"value\": values\n... })\n&gt;&gt;&gt; make_stationary_signals(df, method=\"difference\")\nsignal_id  timestamp     value\n0         abc          1 -6.841017\n1         abc          2  3.967715\n2         abc          3 -1.896646\n3         abc          4 -1.531380\n4         abc          5  1.708821\n..        ...        ...       ...\n193       def         95  0.653840\n194       def         96  0.846767\n195       def         97  5.441443\n196       def         98 -8.955780\n197       def         99  5.397502\n</code></pre>"},{"location":"api/stationarity/#determine_stationary_signals","title":"<code>determine_stationary_signals</code>","text":"<p>Compute the fraction and unique identifiers of signals in the DataFrame that  are statistically stationary.</p>"},{"location":"api/stationarity/#syntax_1","title":"Syntax","text":"<pre><code>def determine_stationary_signals(\ndf: pd.DataFrame,\nalpha: float = 0.05,\nsignal_id: str = \"signal_id\",\ntimestamp: str = \"timestamp\",\nvalue_col: str = \"value\",\n) -&gt; tuple[float, np.ndarray]:\n</code></pre>"},{"location":"api/stationarity/#args_1","title":"Args","text":"<ul> <li><code>df</code> (pd.DataFrame): The input DataFrame containing signal observations.</li> <li><code>alpha</code> (float, optional): The significance level for the stationarity test (default: 0.05).</li> <li><code>signal_id</code> (str, optional): The column name for the signal ID (default: 'signal_id').</li> <li><code>timestamp</code> (str, optional): The column name for the timestamp (default: 'timestamp').</li> <li><code>value_col</code> (str, optional): The column name for the signal values (default: 'value').</li> </ul>"},{"location":"api/stationarity/#returns_1","title":"Returns","text":"<ul> <li><code>tuple[float, np.ndarray]</code>: The fraction of signals that are statistically  stationary and array of signal IDs which are statistically stationary</li> </ul>"},{"location":"api/stationarity/#example_1","title":"Example","text":"<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from cypet.stationarity import determine_stationary_signals\n&gt;&gt;&gt; signal_ids = np.repeat([\"abc\", \"def\"], 100)\n&gt;&gt;&gt; timestamps = np.tile(np.arange(100), 2)\n&gt;&gt;&gt; abc_values = np.linspace(0, 100, 100)\n&gt;&gt;&gt; def_values = np.sin(np.linspace(0, 2 * np.pi, 100))\n&gt;&gt;&gt; values = np.concatenate((abc_values, def_values))\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"signal_id\": signal_ids,\n...     \"timestamp\": timestamps,\n...     \"value\": values\n... })\n&gt;&gt;&gt; stationary_fraction, stationary_signals = determine_stationary_signals(df)\n&gt;&gt;&gt; print(stationary_fraction)\n0.5\n&gt;&gt;&gt; print(stationary_signals)\narray([\"def\"])\n</code></pre>"},{"location":"math_explanations/entropy/","title":"Entropy","text":"<p>Meow</p>"},{"location":"math_explanations/regression/","title":"Regression-Based De-Trending","text":"<p>When de-trending CPET time series signals, we often encounter the need to approximate  an upward within the data. In CPyET, we offer two de-trending approaches: linear  regression and Gaussian processes (GPs).</p>"},{"location":"math_explanations/regression/#linear-regression","title":"Linear Regression","text":"<p>Linear regression is a widely used method for estimating the relationship between  a time series signal \\(\\mathbf{y} \\in \\mathbb{R}^T\\) of length \\(T\\) and a set of  predictive features \\(\\mathbf{X} \\in \\mathbb{R}^{T \\times d}\\) associated with the signal (e.g., thermodynamic work rate, etc.). The goal is to find the optimal coefficients \\(\\beta^*\\)  that minimize the squared Euclidean norm between the observed signal \\(\\mathbf{y}\\) and  the predicted values \\(\\mathbf{X}\\beta\\). Mathematically, this is formulated as:</p> \\[     \\beta^* := \\text{argmin}_{\\beta \\in \\mathbb{R}^d} \\quad \\lVert \\mathbf{y} - \\mathbf{X} \\beta \\rVert_2^2 \\] <p>Fortunately, there exist incredibly efficient and numerically stable algorithms  to find the optimal coefficients \\(\\beta^*\\). Using the optimal \\(\\beta^*\\), we then de-trend the signal by calculating:</p> \\[     \\hat{s}_t := y_t - \\mathbf{x}_t^T \\beta^*, \\quad t = 1, \\ldots, T \\] <p>However, it's important to note that linear regression is limited to approximating  trends that can be expressed as a linear combination of the predictive features  \\(\\mathbf{X}\\). In cases where more complex trends are present, such as non-linear  relationships, linear regression may not provide adequate results. In CPyET, we  provide linear regression de-trending in make_stationary_signals, but we do not recommend using this method for de-trending CPET signals.</p>"},{"location":"math_explanations/regression/#gaussian-process-de-trending-in-cpyet","title":"Gaussian Process De-Trending in CPyET","text":"<p>To overcome the limitations of linear regression, CPyET offers de-trending using  Gaussian processes (GPs). Gaussian processes are powerful non-parametric regression  techniques that model the relationship between inputs and outputs based on the  assumption of a Gaussian process prior over functions. GPs are particularly  well-suited for capturing complex, non-linear relationships and providing  uncertainty estimates. A GP prior is defined as a collection of random variables,  any finite number of which have a joint Gaussian distribution. It can be thought  of as a distribution over functions 1</p> <p>In CPyET, we utilize the radial basis function (RBF) kernel, also known as the  squared exponential or Gaussian kernel, for GP de-trending. The RBF kernel plays  a crucial role in capturing the underlying patterns in the data by specifying  the similarity between input data points.</p> <p>The RBF kernel between two input points, \\(\\mathbf{x}\\) and \\(\\mathbf{x}^\\prime\\) is defined as:</p> \\[     k(\\mathbf{x}, \\mathbf{x}^\\prime) := \\exp \\left(-\\frac{1}{2l^2} \\lVert \\mathbf{x} - \\mathbf{x}^\\prime \\rVert_2^2 \\right) \\] <p>where \\(l &gt; 0\\) defines the length scale hyperparameter, controlling the smoothness of the function.  The RBF kernel captures the notion that inputs close in the input space should have similar outputs. The length scale parameter determines how far-reaching the influence of a data point  is on its neighbors. Additionally, the RBF kernel guarantees that the kernel  matrix \\(\\mathbf{K} := k(\\mathbf{x}, \\mathbf{x}^\\prime) \\ \\ \\forall \\mathbf{x}, \\mathbf{x}^\\prime \\in \\mathbf{X}\\)  is positive definite.</p> <p>To estimate expected value of a set of new points, \\(\\mathbf{X}_*\\), with the associated matrix of  covariance values \\(\\mathbf{K}_*\\), representing the covariance between the new points  \\(\\mathbf{X}_*\\) and all previous samples in \\(\\mathbf{X}\\) we calculate:</p> \\[ \\mathbb{E}[\\mathbf{X}_* \\vert l] := \\mathbf{K}_*^T \\left(\\mathbf{K} + \\sigma^2 \\mathbf{I} \\right)^{-1} \\mathbf{y} \\] <p>In the above expression, \\(\\sigma^2 \\mathbf{I}\\) is added to the kernel matrix to  account for noise in \\(\\mathbf{y}\\) and ensure numerical stability. Instead of  inverting the matrix, \\(\\mathbf{K} + \\sigma^2 \\mathbf{I}\\), which is an \\(\\mathcal{O}(n^3)\\)  complexity operation, because the jittered kernel matrix is positive definite,  we can compute the Cholesky factorization of this matrix to achieve superior computational performance.</p> <p>In CPyET, the estimation process of the expected value for these new points is roughly implemented as follows:</p> <pre><code>K = rbf_kernel(X, X, length_scale=l)\nKstar = rbf_kernel(Xstar, X, length_scale=l)\ndiag(K) += sigma ** 2\nL = scipy.linalg.cho_factor(K, lower=True)\na = scipy.linalg.cho_solve(L, y)\nybar = Kstar.T @ a\n</code></pre> <p>We determine the optimal length scale value, \\(l^*\\), by performing a randomized search cross-validation procedure. Finally, using \\(l^*\\), we de-trend the original time series signal, \\(\\mathbf{y}\\), by calculating:</p> \\[     \\hat{s}_t := y_t - \\mathbb{E}[\\mathbf{x}_t \\vert l^*], \\quad t = 1, \\ldots, T \\] <ol> <li> <p>Williams, Christopher KI, and Carl Edward Rasmussen.  Gaussian processes for machine learning. Vol. 2. No. 3. Cambridge, MA: MIT press, 2006.\u00a0\u21a9</p> </li> </ol>"},{"location":"math_explanations/stationarity/","title":"Stationarity","text":""},{"location":"math_explanations/stationarity/#what-is-stationarity","title":"What is stationarity?","text":"<p>In the context of time series analysis, a stationary time series refers to a  series of observations or data points where statistical properties remain constant over time.  In other words, the behavior and characteristics of the time series do not change as time progresses.</p> <p>There are two main components to consider when determining if a time series is stationary:</p> <ul> <li> <p>Constant Mean: The mean (average) of the time series remains constant over time.  This implies that, on average, the observations do not exhibit any long-term trend  or systematic upward or downward shifts.</p> </li> <li> <p>Constant Variance: The variance (or standard deviation) of the time series  remains constant over time. This indicates that the dispersion or spread of the  observations around the mean does not change with time. It implies that the  fluctuations or variability of the series do not exhibit any systematic change.</p> </li> </ul> <p>Technically speaking, the two above properties, if true, define a weakly stationary signal. To have a strictly stationary signal, it is also necessary to have constant autocovariance. However, for the purposes of CPET variability analysis, this condition is not necessary, and so when we say that a signal is stationary, we mean in the weak sense.</p>"},{"location":"math_explanations/stationarity/#why-do-we-care-if-a-signal-is-stationary","title":"Why do we care if a signal is stationary?","text":"<p>In this package, we primarily refer to variability anlaysis in the context of entropy, specifically sample entropy and permutation entropy. Richman and Moorman1, the researchers who designed sample entropy, and Bandt and Pompe2 (permutation entropy), both assumed as a starting point that the input signal was stationary. Moreover, Chatain et al.3 (and others) have done empirical work demonstrating why stationarity is a necessary condition for valid time series variability analysis. Practicaly speaking, if the signal is non-stationary, such as having an  increasing mean during CPET, it is likely to result in higher entropy values due  to the template matching scheme based on a fixed radial distance  (see Entropy for further explanation on what this means). When it comes to addressing non-stationarity in signals, specifically when the  main issue is a non-constant mean (the most common issue with CPET signals),  there are two common approaches: differencing and de-trending.4</p>"},{"location":"math_explanations/stationarity/#differencing","title":"Differencing","text":"<p>Differencing involves creating a new signal by subtracting the previous observation  from the current observation. Let \\(\\mathbf{y} \\in \\mathbb{R}^T\\) define a time series signal of length, \\(T\\). Formally, differencing is defined as:</p> \\[     \\tilde s_t := y_t - y_{t-1}, \\quad t = 2, \\ldots, T \\] <p>The rationale behind differencing stems from autoregressive processes and random walk theory.5 This approach is relatively straightforward to implement  (see: make_stationary_signals for details), and one can assess the statistical stationarity of the differenced signal,  \\(\\tilde{\\mathbf{s}}\\), at a given significance level, \\(\\alpha\\), using the  Augmented Dickey-Fuller (ADF) test.6</p>"},{"location":"math_explanations/stationarity/#de-trending","title":"De-Trending","text":"<p>Alternatively, one can estimate and remove the trend present in a non-stationary signal. This approach offers various implementations, with linear or polynomial regression  being the most common.7 Any algorithm that estimates \\(\\hat{y}_t := f(\\mathbf{x}_t)\\), where \\(\\mathbf{x}_t\\) represents a set of predictive features and \\(f(\\cdot)\\) is a regression function, can be used to de-trend the original signal, \\(\\mathbf{y}\\), by calculating:</p> \\[     \\hat{s}_t := y_t - \\hat{y}_t, \\quad t = 1, \\ldots, T \\] <p>In CPyET, we provide two options for de-trending: a standard linear regression function and a radial basis function Gaussian process (GP). If you need to de-trend a  non-stationary signal, we highly recommend using the GP implementation. The GP  method provided in make_stationary_signals  has the advantage of automatically accommodating nonlinear trends. Our empirical  results demonstrate that it significantly increases the proportion of statistically  stationary signals. For a more in depth discussion of the mathematical aspects of  linear regression and GPs, please refer to the Regression section.</p>"},{"location":"math_explanations/stationarity/#should-i-choose-differencing-or-de-trending","title":"Should I choose differencing or de-trending?","text":"<p>Both methods are implemented in make_stationary_signals and have strong theoretical and empirical bases. In practice, we recommend trying both approaches and seeing which method yields a larger proportion of statistically stationary signals.</p> <ol> <li> <p>Richman, Joshua S., and J. Randall Moorman. \"Physiological time-series analysis using approximate entropy and sample entropy.\"  American journal of physiology-heart and circulatory physiology 278.6 (2000): H2039-H2049.\u00a0\u21a9</p> </li> <li> <p>Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\"  Physical review letters 88.17 (2002): 174102.\u00a0\u21a9</p> </li> <li> <p>Chatain, Cyril, et al. \"Effects of nonstationarity on muscle force signals regularity during a fatiguing motor task.\"  IEEE Transactions on Neural Systems and Rehabilitation Engineering 28.1 (2019): 228-237.\u00a0\u21a9</p> </li> <li> <p>Berry, Nathaniel T., et al. \"Heart rate dynamics during acute recovery from maximal aerobic exercise in young adults.\"  Frontiers in Physiology 12 (2021): 627320.\u00a0\u21a9</p> </li> <li> <p>Gonedes, Nicholas J., and Harry V. Roberts. \"Differencing of random walks and near random walks.\"  Journal of Econometrics 6.3 (1977): 289-308.\u00a0\u21a9</p> </li> <li> <p>Dickey, David A., and Wayne A. Fuller. \"Distribution of the estimators for autoregressive time series with a unit root.\"  Journal of the American statistical association 74.366a (1979): 427-431.\u00a0\u21a9</p> </li> <li> <p>Berry, Nathaniel T., Laurie Wideman, and Christopher K. Rhea. \"Variability and complexity of non-stationary functions: methods for post-exercise HRV.\"  Nonlinear Dynamics, Psychology &amp; Life Sciences 24.4 (2020).\u00a0\u21a9</p> </li> </ol>"}]}