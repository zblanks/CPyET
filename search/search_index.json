{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#overview-and-introduction","title":"Overview and Introduction","text":"<p>Welcome to the EristroPy package documentation! This document provides an overview of EristroPy,  a powerful framework for working time series signals via entropy using Python. </p> <p>The EristroPy package is designed to streamline the variability analysis of signals in Python. It provides end-to-end functionality, starting from constructing stationary signals  to determining appropriate metric parameters and efficiently computing entropy  and variability measures. By leveraging EristroPy, researchers and practitioners can  focus on the analysis and interpretation of time series data, rather than spending time  and effort on the intricate details of signal processing and analysis.</p> <p>To the best of our knowledge, EristroPy is the only existing solution in Python that  offers all the necessary functionality for valid and reproducible entropy analysis  using novel and scalable heuristics. Its features and benefits enable researchers  to perform comprehensive variability analysis, gain valuable insights, and  contribute to advancements in the field of cardiopulmonary exercise testing.</p>"},{"location":"#features-benefits","title":"Features &amp; Benefits","text":"<p>EristroPy offers a range of features and benefits that facilitate the analysis of time series signals:</p> <ul> <li>Automatic Signal Stationarity: EristroPy enables seamless construction of stationary signals,  a necessary condition for valid entropy and variability analysis.  It incorporates two common techniques, differencing and de-trending,  and performs statistical stationarity checks to ensure that the dataset contains valid signals.</li> <li>Scalable Entropy Calculations: EristroPy provides efficient implementations of  sample and permutation entropy. Leveraging Numba's just-in-time compilation scheme,  EristroPy ensures fast and scalable computations, allowing researchers to focus on  the analysis rather than the intricacies of the calculations.</li> <li>Optimal Parameter Selction: Determining appropriate parameter settings for  entropy measures can be challenging. EristroPy takes the guesswork out by providing  reasonable recommendations based on rigorous, nonparametric statistical approaches.  These recommendations empower researchers to confidently choose suitable parameters for their analysis.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>You can install EristroPy by using pip:</p> <pre><code>pip install eristropy\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>To start using EristroPy in your Python project, import it as follows:</p> <pre><code>import eristropy\n</code></pre>"},{"location":"#license","title":"License","text":"<p>EristroPy is released under the MIT License.</p> <p>The MIT License (MIT)</p> <p>Copyright (c) 2023 Zachary Blanks</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/sample_entropy/","title":"SampleEntropy","text":""},{"location":"api/sample_entropy/#overview","title":"Overview","text":"<p>The <code>SampleEntropy</code> class computes the Sample Entropy (SampEn) of multiple signals and finds the optimal SampEn parameters using a regularized MSE objective function and the Optuna Bayesian optimization framework with the Tree-based Parzen Estimator (TPE) surrogate function. </p> <p>For more details on this optimization procedure, see Z. Blanks et al., Optimal Sample Entropy Parameter Selection for Short Time Series Signals via Bayesian Optimization, 2023.</p> <p>The main methods are <code>find_optimal_sampen_params()</code> for performing the optimization and <code>compute_all_sampen()</code> for computing the SampEn of all signals with the optimized or user-provided parameters.</p>"},{"location":"api/sample_entropy/#attributes","title":"Attributes","text":"<ul> <li><code>df</code>: (<code>pd.DataFrame</code>) The DataFrame containing the signals. Must contain columns for the signal_id, timestamp, and signal value at each timestamp.</li> <li><code>signal_id</code>: (<code>str</code>, optional) Column name in <code>df</code> containing the signal IDs. Default is 'signal_id'.</li> <li><code>timestamp</code>: (<code>str</code>, optional) Column name in <code>df</code> containing the timestamps. Default is 'timestamp'.</li> <li><code>value_col</code>: (<code>str</code>, optional) Column name in <code>df</code> containing the values. Default is 'value'.</li> <li><code>objective</code>: (<code>str</code>, optional) Objective function to minimize. Default is 'mse'. Choices are 'mse' and 'sampen_eff'.</li> <li><code>n_boot</code>: (<code>int</code>, optional) Number of bootstrap samples to use in the estimation. Default is 100.</li> <li><code>n_trials</code>: (<code>int</code>, optional) Number of trials for the optimization. Default is 100.</li> <li><code>random_seed</code>: (<code>int</code>, optional) Seed for the random number generator. Default is None.</li> <li><code>r_range</code>: (<code>tuple[float, float]</code>, optional) Tuple specifying the range of \\(r\\) values for the optimization. Default is (0.10, 0.50).</li> <li><code>m_range</code>: (<code>tuple[int, int]</code>, optional) Tuple specifying the range of \\(m\\) values for the optimization. Default is (1, 3).</li> <li><code>p_range</code>: (<code>tuple[float, float]</code>, optional) Tuple specifying the range of \\(p\\) values for the stationary bootstrap. Default is (0.01, 0.99).</li> <li><code>lam</code>: (<code>float</code>, optional) The trade-off parameter between the \\(r\\)-based penalization. Default is 0.33.</li> <li><code>r</code>: (<code>float</code>, optional) User-provided value for \\(r\\). Default is None.</li> <li><code>m</code>: (<code>int</code>, optional) User-provided value for \\(m\\). Default is None.</li> <li><code>p</code>: (<code>float</code>, optional) User-provided value for \\(p\\). Default is None.</li> </ul>"},{"location":"api/sample_entropy/#methods","title":"Methods","text":""},{"location":"api/sample_entropy/#find_optimal_sampen_params","title":"<code>find_optimal_sampen_params</code>","text":"<p>Finds the optimal \\((m, r)\\) SampEn parameters for the input signal set.</p>"},{"location":"api/sample_entropy/#notes","title":"Notes","text":"<p>This method uses the Optuna library for optimizing the parameters \\((m, r, p)\\) using a TPE surrogate function.</p>"},{"location":"api/sample_entropy/#example","title":"Example","text":"<pre><code>&gt;&gt;&gt; sampen = SampleEntropy(df, n_trials=50)\n&gt;&gt;&gt; sampen.find_optimal_sampen_params()\n</code></pre>"},{"location":"api/sample_entropy/#compute_all_sampen","title":"<code>compute_all_sampen</code>","text":"<p>Computes the SampEn of the input signal set given either the provided or optimized values of \\((m, r)\\).</p>"},{"location":"api/sample_entropy/#parameters","title":"Parameters","text":"<ul> <li><code>optimize</code>: (<code>bool</code>, optional) If True, optimize the SampEn parameters before computing the SampEn for all signals. Defaults to False.</li> <li><code>estimate_uncertainty</code>: (<code>bool</code>, optional) If True, estimates the SE(SampEn) for the given or optimized \\((m, r)\\) values. Defaults to False.</li> </ul>"},{"location":"api/sample_entropy/#returns","title":"Returns","text":"<ul> <li><code>pd.DataFrame</code>: SampEn estimates given \\((m, r)\\) for all signals in the data.</li> </ul>"},{"location":"api/sample_entropy/#example_1","title":"Example","text":"<pre><code>&gt;&gt;&gt; sampen = SampleEntropy(df)\n&gt;&gt;&gt; results = sampen.compute_all_sampen(optimize=True)\n</code></pre>"},{"location":"api/sample_entropy/#get_optimization_results","title":"<code>get_optimization_results</code>","text":"<p>Return a DataFrame of the optimization results.</p>"},{"location":"api/sample_entropy/#parameters_1","title":"Parameters","text":"<ul> <li><code>attrs</code>: (tuple, optional) Attributes of the optuna trials to include in the dataframe. By default it includes the trial number, value of the objective function, and parameters used. Refer to optuna documentation for other options.</li> </ul>"},{"location":"api/sample_entropy/#returns_1","title":"Returns","text":"<ul> <li><code>pd.DataFrame</code>: DataFrame of the optimization trials.</li> </ul>"},{"location":"api/sample_entropy/#example_2","title":"Example","text":"<pre><code>&gt;&gt;&gt; sampen = SampleEntropy(df)\n&gt;&gt;&gt; sampen.find_optimal_sampen_params()\n&gt;&gt;&gt; optimization_results = sampen.get_optimization_results()\n</code></pre>"},{"location":"api/stationarity/","title":"StationarySignals","text":""},{"location":"api/stationarity/#overview","title":"Overview","text":"<p>The <code>StationarySignals</code> class is designed to transform a set of time-series signals into stationary signals.  This is an essential pre-processing step in many time-series analysis tasks.</p>"},{"location":"api/stationarity/#attributes","title":"Attributes","text":"<ul> <li><code>df</code>: (<code>pd.DataFrame</code>) The input DataFrame containing the signals.</li> <li><code>signal_id</code>: (<code>str</code>, optional) Column name in <code>df</code> containing the signal IDs. Default is 'signal_id'.</li> <li><code>timestamp</code>: (<code>str</code>, optional) Column name in <code>df</code> containing the timestamps. Default is 'timestamp'.</li> <li><code>value_col</code>: (<code>str</code>, optional) Column name in <code>df</code> containing the values. Default is 'value'.</li> <li><code>method</code>: (<code>str</code>, optional) The method to use for making signals stationary. Default is 'difference'. Choices are 'difference' and 'detrend'.</li> <li><code>detrend_type</code>: (<code>str</code>, optional) The type of detrending to use if <code>method</code> is 'detrend'. Default is 'gp'. Choices are 'gp' and 'lr'.</li> <li><code>alpha</code>: (<code>float</code>, optional) Significance level for the Augmented Dickey-Fuller test. Default is 0.05.</li> <li><code>random_seed</code>: (<code>int</code>, optional) Seed for the random number generator. Default is None.</li> <li><code>ls_range</code>: (<code>tuple</code>, optional) Tuple specifying the range of length-scales for the Gaussian process. Default is (10.0, 100.0).</li> <li><code>ls_values</code>: (<code>np.ndarray</code>, optional) Array of specific length scale values. Default is None.</li> <li><code>n_searches</code>: (<code>int</code>, optional) Number of searches for Gaussian process hyperparameters. Default is 10.</li> <li><code>n_splits</code>: (<code>int</code>, optional) Number of splits for cross-validation. Default is 5.</li> <li><code>eps</code>: (<code>float</code>, optional) Tolerance for the difference. Default is 1e-6.</li> <li><code>gp_implementation</code>: (<code>str</code>, optional) Implementation to use for the Gaussian process. Default is 'numba'. Choices are 'sklearn' and 'numba'.</li> <li><code>sklearn_scoring</code>: (<code>str</code>, optional) Scoring method when using 'sklearn' for Gaussian process. Default is 'neg_mean_squared_error'.</li> <li><code>normalize_signals</code>: (<code>bool</code>, optional) Whether to normalize signals to zero mean and unit variance. Default is True.</li> </ul>"},{"location":"api/stationarity/#methods","title":"Methods","text":""},{"location":"api/stationarity/#make_stationary_signals","title":"<code>make_stationary_signals()</code>","text":"<p>Creates stationary signals at specified statistical level, \\(\\alpha\\).</p>"},{"location":"api/stationarity/#notes","title":"Notes","text":"<p>We have implemented an RBF-based GP detrending in Numba using a standard Cholesky factorization solution. This method, on average, is faster than the Scikit-Learn implementation. The benefit will become more noticeable as the number of unique signals increases. However, this implementation is less numerically stable and less well-tested than the Scikit-Learn version. You can control which version you use via the argument <code>gp_implementation</code>.</p>"},{"location":"api/stationarity/#example","title":"Example","text":"<pre><code>&gt;&gt;&gt; signal_ids = np.repeat([\"abc\", \"def\"], 100)\n&gt;&gt;&gt; timestamps = np.tile(np.arange(100), 2)\n&gt;&gt;&gt; rng = np.random.default_rng(17)\n&gt;&gt;&gt; abc_values = rng.uniform(-5, 5, size=(100,))\n&gt;&gt;&gt; def_values = rng.uniform(-5, 5, size=(100,))\n&gt;&gt;&gt; values = np.concatenate((abc_values, def_values))\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"signal_id\": signal_ids,\n...     \"timestamp\": timestamps,\n...     \"value\": values\n... })\n&gt;&gt;&gt; signals = StationarySignals(df, method='difference', normalize_signals=False)\n&gt;&gt;&gt; signals.make_stationary_signals()\nsignal_id  timestamp     value\n0         abc          1 -6.841017\n1         abc          2  3.967715\n2         abc          3 -1.896646\n3         abc          4 -1.531380\n4         abc          5  1.708821\n..        ...        ...       ...\n193       def         95  0.653840\n194       def         96  0.846767\n195       def         97  5.441443\n196       def         98 -8.955780\n197       def         99  5.397502\n</code></pre>"},{"location":"math_explanations/entropy/","title":"Entropy","text":""},{"location":"math_explanations/entropy/#shannon-entropy","title":"Shannon Entropy","text":"<p>Information theory is a discipline intersecting mathematics and computer science that focuses on the quantification, storage, and communication of information. One of its fundamental tenets, entropy, was introduced by Claude Shannon in 1948<sup>1</sup>. It quantifies the average amount of information or uncertainty inherent in a random variable or data source. If we consider a discrete random variable, \\(X\\), with support \\(S_X\\) and a probability mass function, \\(p(x)\\), the entropy of this random variable, \\(X\\), can be calculated as:</p> \\[     H(X) = -\\sum_{x \\in S_X} p(x) \\log_2 p(x) \\] <p>Higher entropy equates to greater uncertainty, and lower entropy implies increased predictability.</p>"},{"location":"math_explanations/entropy/#entropy-of-time-series-signals","title":"Entropy of Time Series Signals","text":"<p>In Shannon's groundbreaking work, he introduced a rigorous definition of uncertainty for static systems. However, time series signals represent dynamic systems that evolve over time due to perturbations and inputs. Therefore, it is crucial to develop a concept of entropy that captures the complexity and uncertainty of these dynamic systems.</p> <p>In 1958 and 1959, Kolmogorov and Sinai developed such a measure, now coined Kolmogorov-Sinai (KS) entropy<sup>2</sup>. KS entropy is a measure of the complexity or randomness present in a dynamical system or time series. It quantifies the rate at which information is generated by the system over time. While KS entropy provides a rigorous and mathematically precise definition of entropy applied to time series signals, it is often not a computationally tractable one<sup>3</sup>. It is for this reason that researchers have developed alternate measures, such as sample entropy (SampEn) and permutation entropy (PermEn).</p>"},{"location":"math_explanations/entropy/#sample-entropy","title":"Sample Entropy","text":"<p>SampEn is a measure used to estimate the complexity of time series data. Developed by Richman and Moorman<sup>4</sup>, it builds upon Pincus's approximate entropy<sup>5</sup>. The concept of SampEn revolves around the idea of template matching in a signal. A template is a subset of consecutive data points from the signal.</p> <p>Define a time series signal of length \\(N\\) as \\(\\mathbf{x} \\in \\mathbb{R}^N\\). A template of length \\(m\\) (known as the \"embedding dimension\") is denoted by \\(\\mathbf{u}_m(i) = (x_i, \\ldots, x_{i + m - 1})\\). We compare these templates and consider them a match if the distance between them is less than a defined radius, \\(r\\). Specifically, a match occurs when \\(\\lVert \\mathbf{u}_m(i) - \\mathbf{u}_m(j)\\rVert_p \\leq r\\), where \\(p\\) is the order of the norm (typically \\(p = \\infty\\) for the L-infinity norm denoting the maximum absolute difference between the elements of two templates) and \\(r &gt; 0\\) is the predefined radius. Unlike ApEn, SampEn does not consider self-matches.</p> <p>To calculate signal regularity, we compute two quantities, \\(B^m(r)\\) and \\(A^m(r)\\). \\(B^m(r)\\), defined as:</p> \\[     B^m(r) = \\frac{1}{Z(N, m)} \\sum_{i = 1}^{N-m} \\sum_{\\substack{j=1\\ j\\neq i}}^{N-m} \\mathbf{1}\\left[\\lVert \\mathbf{u}_m(i) - \\mathbf{u}_m(j)\\rVert_p \\leq r\\right], \\] <p>is the probability of the signal remaining within a radius, \\(r\\), for \\(m\\) steps. Similarly, \\(A^m(r)\\), given by:</p> \\[     A^m(r) = \\frac{1}{Z(N, m)} \\sum_{i = 1}^{N-m} \\sum_{\\substack{j=1\\ j\\neq i}}^{N-m} \\mathbf{1}\\left[\\lVert\\mathbf{u}_{m+1}(i) - \\mathbf{u}_{m+1}(j)\\rVert_p \\leq r\\right], \\] <p>is the probability that the signal stays within the same radius for an additional step (\\(m + 1\\) steps in total). In both expressions, \\(\\mathbf{1}\\left[\\cdot \\right]\\) denotes the indicator function and \\(Z(N, m)\\) is a normalization constant to ensure valid probabilities. With these values, we define SampEn as the negative logarithm of the conditional probability that a sequence will remain within radius \\(r\\) for \\(m + 1\\) steps, given it has stayed within \\(r\\) for \\(m\\) steps:</p> \\[     \\text{SampEn}(\\mathbf{x}, m, r) = -\\log\\left( \\frac{A^m(r)}{B^m(r)}\\right). \\] <p>However, if no matches are found at radius \\(r\\) across the signal (i.e., \\(B^m(r) = 0\\)), \\(\\text{SampEn}(\\mathbf{x}, m, r)\\) is undefined.</p> <ol> <li> <p>Shannon, Claude Elwood. \"A mathematical theory of communication.\" The Bell system technical journal 27.3 (1948): 379-423.\u00a0\u21a9</p> </li> <li> <p>Shiryayev, A. N. \"New metric invariant of transitive dynamical systems and automorphisms of Lebesgue spaces.\" Selected Works of AN Kolmogorov: Volume III: Information Theory and the Theory of Algorithms (1993): 57-61.\u00a0\u21a9</p> </li> <li> <p>Kantz, Holger, and Thomas Schreiber. Nonlinear time series analysis. Vol. 7. Cambridge university press, 2004.\u00a0\u21a9</p> </li> <li> <p>Richman, Joshua S., and J. Randall Moorman. \"Physiological time-series analysis using approximate entropy and sample entropy.\" American journal of physiology-heart and circulatory physiology 278.6 (2000): H2039-H2049.\u00a0\u21a9</p> </li> <li> <p>Pincus, Steven M. \"Approximate entropy as a measure of system complexity.\" Proceedings of the National Academy of Sciences 88.6 (1991): 2297-2301.\u00a0\u21a9</p> </li> </ol>"},{"location":"math_explanations/regression/","title":"Regression-Based De-Trending","text":"<p>In EristroPy, we offer two de-trending approaches: linear  regression and Gaussian processes (GPs).</p>"},{"location":"math_explanations/regression/#linear-regression","title":"Linear Regression","text":"<p>Linear regression is a widely used method for estimating the relationship between  a time series signal \\(\\mathbf{x} \\in \\mathbb{R}^N\\) of length \\(N\\) and a set of  predictive features \\(\\mathbf{\\Theta} \\in \\mathbb{R}^{N \\times d}\\) associated with the signal (e.g., thermodynamic work rate, etc.). The goal is to find the optimal coefficients \\(\\beta^*\\)  that minimize the squared Euclidean norm between the observed signal \\(\\mathbf{x}\\) and  the predicted values \\(\\mathbf{\\Theta}\\beta\\). Mathematically, this is formulated as:</p> \\[     \\beta^* := \\text{argmin}_{\\beta \\in \\mathbb{R}^d} \\quad \\lVert \\mathbf{x} - \\mathbf{\\Theta} \\beta \\rVert_2^2 \\] <p>Fortunately, there exist incredibly efficient and numerically stable algorithms  to find the optimal coefficients \\(\\beta^*\\). Using the optimal \\(\\beta^*\\), we then de-trend the signal by calculating:</p> \\[     \\widetilde{x}_t := x_t - \\theta_t^T \\beta^*, \\quad t = 1, \\ldots, T \\] <p>However, it's important to note that linear regression is limited to approximating  trends that can be expressed as a linear combination of the predictive features  \\(\\mathbf{\\Theta}\\). In cases where more complex trends are present, such as non-linear  relationships, linear regression may not provide adequate results. In EristroPy, we  provide linear regression de-trending in make_stationary_signals, but we do not recommend using this method for de-trending signals.</p>"},{"location":"math_explanations/regression/#gaussian-process-de-trending-in-eristropy","title":"Gaussian Process De-Trending in EristroPy","text":"<p>To overcome the limitations of linear regression, EristroPy offers de-trending using  GPs. Gaussian processes are powerful non-parametric regression  techniques that model the relationship between inputs and outputs based on the  assumption of a Gaussian process prior over functions. GPs are particularly  well-suited for capturing complex, non-linear relationships and providing  uncertainty estimates. A GP prior is defined as a collection of random variables,  any finite number of which have a joint Gaussian distribution. It can be thought  of as a distribution over functions <sup>1</sup></p> <p>In EristroPy, we utilize the radial basis function (RBF) kernel, also known as the  squared exponential or Gaussian kernel, for GP de-trending. The RBF kernel plays  a crucial role in capturing the underlying patterns in the data by specifying  the similarity between input data points.</p> <p>The RBF kernel between two input points, \\(\\theta\\) and \\(\\theta^\\prime\\) is defined as:</p> \\[     k(\\theta, \\theta^\\prime) := \\exp \\left(-\\frac{1}{2l^2} \\lVert \\theta - \\theta^\\prime \\rVert_2^2 \\right) \\] <p>where \\(l &gt; 0\\) defines the length scale hyperparameter, controlling the smoothness of the function.  The RBF kernel captures the notion that inputs close in the input space should have similar outputs. The length scale parameter determines how far-reaching the influence of a data point  is on its neighbors. Additionally, the RBF kernel guarantees that the kernel  matrix \\(\\mathbf{K} := k(\\theta, \\theta^\\prime) \\ \\ \\forall \\theta, \\theta^\\prime \\in \\mathbf{\\Theta}\\)  is positive definite.</p> <p>To estimate expected value of a set of new points, \\(\\mathbf{\\Theta}_*\\), with the associated matrix of  covariance values \\(\\mathbf{K}_*\\), representing the covariance between the new points  \\(\\mathbf{\\Theta}_*\\) and all previous samples in \\(\\mathbf{\\Theta}\\) we calculate:</p> \\[ \\mathbb{E}[\\mathbf{\\Theta}_* \\vert l] := \\mathbf{K}_*^T \\left(\\mathbf{K} + \\sigma^2 \\mathbf{I} \\right)^{-1} \\mathbf{y} \\] <p>In the above expression, \\(\\sigma^2 \\mathbf{I}\\) is added to the kernel matrix to  account for noise in \\(\\mathbf{y}\\) and ensure numerical stability. Instead of  inverting the matrix, \\(\\mathbf{K} + \\sigma^2 \\mathbf{I}\\), which is an \\(\\mathcal{O}(n^3)\\)  complexity operation, because the jittered kernel matrix is positive definite,  we can compute the Cholesky factorization of this matrix to achieve superior computational performance.</p> <p>In EristroPy, the estimation process of the expected value for these new points is roughly implemented as follows:</p> <pre><code>K = rbf_kernel(theta, theta, length_scale=l)\nKstar = rbf_kernel(theta_star, theta, length_scale=l)\ndiag(K) += sigma ** 2\nL = scipy.linalg.cho_factor(K, lower=True)\na = scipy.linalg.cho_solve(L, y)\nxbar = Kstar.T @ a\n</code></pre> <p>We determine the optimal length scale value, \\(l^*\\), by performing a randomized search cross-validation procedure. Finally, using \\(l^*\\), we de-trend the original time series signal, \\(\\mathbf{y}\\), by calculating:</p> \\[     \\widetilde{x}_t := x_t - \\mathbb{E}[\\theta_t \\vert l^*], \\quad t = 1, \\ldots, T \\] <ol> <li> <p>Williams, Christopher KI, and Carl Edward Rasmussen.  Gaussian processes for machine learning. Vol. 2. No. 3. Cambridge, MA: MIT press, 2006.\u00a0\u21a9</p> </li> </ol>"},{"location":"math_explanations/stationarity/","title":"Stationarity","text":""},{"location":"math_explanations/stationarity/#what-is-stationarity","title":"What is stationarity?","text":"<p>In the context of time series analysis, a stationary time series refers to a  series of observations or data points where statistical properties remain constant over time.  In other words, the behavior and characteristics of the time series do not change as time progresses.</p> <p>There are two main components to consider when determining if a time series is stationary:</p> <ul> <li> <p>Constant Mean: The mean (average) of the time series remains constant over time.  This implies that, on average, the observations do not exhibit any long-term trend  or systematic upward or downward shifts.</p> </li> <li> <p>Constant Variance: The variance (or standard deviation) of the time series  remains constant over time. This indicates that the dispersion or spread of the  observations around the mean does not change with time. It implies that the  fluctuations or variability of the series do not exhibit any systematic change.</p> </li> </ul> <p>Technically speaking, the two above properties, if true, define a weakly stationary signal. To have a strictly stationary signal, it is also necessary to have constant autocovariance. However, for the purposes of most entropy analyses, this condition is not necessary, and so when we say that a signal is stationary, we mean in the weak sense.</p>"},{"location":"math_explanations/stationarity/#why-do-we-care-if-a-signal-is-stationary","title":"Why do we care if a signal is stationary?","text":"<p>In this package, we primarily refer to variability anlaysis in the context of entropy, specifically sample entropy and permutation entropy. Richman and Moorman<sup>1</sup>, the researchers who designed sample entropy, and Bandt and Pompe<sup>2</sup> (permutation entropy), both assumed as a starting point that the input signal was stationary. Moreover, Chatain et al.<sup>3</sup> (and others) have done empirical work demonstrating why stationarity is a necessary condition for valid time series variability analysis. Practicaly speaking, if the signal is non-stationary, such as having an  increasing mean during cardiopulmonary exercise testing (CPET), it is likely to  result in higher entropy values due to the template matching scheme based on a  fixed radial distance (see Entropy for further explanation on what this means). When it comes to addressing non-stationarity in signals, specifically when the  main issue is a non-constant mean, there are two common approaches: differencing and de-trending.<sup>4</sup></p>"},{"location":"math_explanations/stationarity/#differencing","title":"Differencing","text":"<p>Differencing involves creating a new signal by subtracting the previous observation  from the current observation. Let \\(\\mathbf{x} \\in \\mathbb{R}^N\\) define a time series signal of length, \\(N\\). Formally, differencing is defined as:</p> \\[     \\widetilde{x}_t := x_t - x_{t-1}, \\quad t = 2, \\ldots, N \\] <p>The rationale behind differencing stems from autoregressive processes and random walk theory.<sup>5</sup> This approach is relatively straightforward to implement  (see: make_stationary_signals for details), and one can assess the statistical stationarity of the differenced signal,  \\(\\widetilde{\\mathbf{x}}\\), at a given significance level, \\(\\alpha\\), using the  Augmented Dickey-Fuller (ADF) test.<sup>6</sup></p>"},{"location":"math_explanations/stationarity/#de-trending","title":"De-Trending","text":"<p>Alternatively, one can estimate and remove the trend present in a non-stationary signal. This approach offers various implementations, with linear or polynomial regression  being the most common.<sup>7</sup> Any algorithm that estimates \\(\\hat{x}_t := f(\\theta_t)\\), where \\(\\theta_t\\) represents a set of predictive features and \\(f(\\cdot)\\) is a regression function, can be used to de-trend the original signal, \\(\\mathbf{y}\\), by calculating:</p> \\[     \\widetilde{x}_t := x_t - f(\\theta_t), \\quad t = 1, \\ldots, T \\] <p>In EristroPy, we provide two options for de-trending: a standard linear regression function and a radial basis function Gaussian process (GP). If you need to de-trend a  non-stationary signal, we highly recommend using the GP implementation. The GP  method provided in make_stationary_signals  has the advantage of automatically accommodating nonlinear trends. Our empirical  results demonstrate that it significantly increases the proportion of statistically  stationary signals. For a more in depth discussion of the mathematical aspects of  linear regression and GPs, please refer to the Regression section.</p>"},{"location":"math_explanations/stationarity/#should-i-choose-differencing-or-de-trending","title":"Should I choose differencing or de-trending?","text":"<p>Both methods are implemented in make_stationary_signals and have strong theoretical and empirical bases. In practice, we recommend trying both approaches and seeing which method yields a larger proportion of statistically stationary signals.</p> <ol> <li> <p>Richman, Joshua S., and J. Randall Moorman. \"Physiological time-series analysis using approximate entropy and sample entropy.\"  American journal of physiology-heart and circulatory physiology 278.6 (2000): H2039-H2049.\u00a0\u21a9</p> </li> <li> <p>Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\"  Physical review letters 88.17 (2002): 174102.\u00a0\u21a9</p> </li> <li> <p>Chatain, Cyril, et al. \"Effects of nonstationarity on muscle force signals regularity during a fatiguing motor task.\"  IEEE Transactions on Neural Systems and Rehabilitation Engineering 28.1 (2019): 228-237.\u00a0\u21a9</p> </li> <li> <p>Berry, Nathaniel T., et al. \"Heart rate dynamics during acute recovery from maximal aerobic exercise in young adults.\"  Frontiers in Physiology 12 (2021): 627320.\u00a0\u21a9</p> </li> <li> <p>Gonedes, Nicholas J., and Harry V. Roberts. \"Differencing of random walks and near random walks.\"  Journal of Econometrics 6.3 (1977): 289-308.\u00a0\u21a9</p> </li> <li> <p>Dickey, David A., and Wayne A. Fuller. \"Distribution of the estimators for autoregressive time series with a unit root.\"  Journal of the American statistical association 74.366a (1979): 427-431.\u00a0\u21a9</p> </li> <li> <p>Berry, Nathaniel T., Laurie Wideman, and Christopher K. Rhea. \"Variability and complexity of non-stationary functions: methods for post-exercise HRV.\"  Nonlinear Dynamics, Psychology &amp; Life Sciences 24.4 (2020).\u00a0\u21a9</p> </li> </ol>"},{"location":"usage/sample_entropy_analysis/","title":"Sample Entropy Analysis Example","text":"<p>This section provides a practical guide to conducting a valid entropy analysis using the <code>StationarySignals</code> and <code>SampleEntropy</code> classes in this package. Below are the steps to accomplish this:</p>"},{"location":"usage/sample_entropy_analysis/#preliminary-set-up","title":"Preliminary Set-Up","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom eristropy.stationarity import StationarySiganls\nfrom eristropy.sample_entropy import SampleEntropy\nfrom eristropy import StationarySignals, SampleEntropy\n</code></pre> <p>Suppose we have time series signals, here we create synthetic signals for demonstration purposes.</p> <pre><code>signal_ids = np.repeat([\"signal_1\", \"signal_2\"], 100)\ntimestamps = np.tile(np.arange(100), 2)\nrng = np.random.default_rng(17)\nsignal_1_values = rng.uniform(-5, 5, size=(100,))\nsignal_2_values = rng.uniform(-5, 5, size=(100,))\nvalues = np.concatenate((signal_1_values, signal_2_values))\ndf = pd.DataFrame({\n\"signal_id\": signal_ids,\n\"timestamp\": timestamps,\n\"value\": values\n})\n</code></pre>"},{"location":"usage/sample_entropy_analysis/#ensuring-stationary-signals","title":"Ensuring Stationary Signals","text":"<p>Before computing SampEn, it's crucial to ensure that the signals are stationary (see Stationarity for more details). This is where <code>StationarySignals</code> comes in handy.</p> <pre><code>signals = StationarySignals(df, method='difference')\nstationary_df = signals.make_stationary_signals()\n</code></pre> <p>In this example, we use the \"differencing\"-based approach, but we also allow users to de-trend the signals if desired.</p>"},{"location":"usage/sample_entropy_analysis/#find-optimal-sampen-parameters-compute-sampen","title":"Find Optimal SampEn Parameters &amp; Compute SampEn","text":"<p>Now, that we have weakly stationary signals, we can compute the SampEn for all unique signals in the dataset. You can also estimate the uncertainty if needed.</p> <pre><code>sampen = SampleEntropy(stationary_df)\nresult_df = sampen.compute_all_sampen(optimize=True, estimate_uncertainty=True)\n</code></pre> <p>The <code>result_df</code> will contain SampEn estimates from the optimal \\((m, r)\\) combination for each unique signal, which you can then analyze further.</p> <pre><code>print(result_df)\n</code></pre>"}]}